name: Match Scraper

on:
  # run every 12 hours (2x per day = ~2700 min/month with 45min runs)
  schedule:
    - cron: '0 */12 * * *'
  # allow manual trigger
  workflow_dispatch:
    inputs:
      reset_state:
        description: 'Reset scraper state before running'
        required: false
        default: 'false'
        type: boolean
      duration_minutes:
        description: 'How long to run the scraper (minutes)'
        required: false
        default: '45'
        type: string

env:
  RIOT_API_KEY: ${{ secrets.RIOT_API_KEY }}
  NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
  NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
  SUPABASE_SECRET_KEY: ${{ secrets.SUPABASE_SECRET_KEY }}
  SCRAPER_THROTTLE: '50'
  USE_REDIS_RATE_LIMIT: 'true'
  UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
  UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}

permissions:
  actions: write
  contents: read

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Check if paused
        env:
          PAUSED: ${{ secrets.SCRAPER_PAUSED }}
        run: |
          if [ "$PAUSED" = "true" ]; then
            echo "Scraper is PAUSED - set SCRAPER_PAUSED secret to empty or remove it to resume"
            exit 1
          fi
          echo "Scraper is active"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Restore scraper state from cache
        uses: actions/cache/restore@v4
        with:
          path: |
            scripts/scraper-state.json
            scripts/match-cache.json
          key: scraper-state-latest
          restore-keys: |
            scraper-state-

      - name: Reset scraper state (if requested)
        if: ${{ github.event.inputs.reset_state == 'true' }}
        run: rm -f scripts/scraper-state.json scripts/match-cache.json

      - name: Show initial state
        run: |
          echo "========================================"
          echo "Scraper State Before Run"
          echo "========================================"
          if [ -f scripts/scraper-state.json ]; then
            echo "State file exists:"
            cat scripts/scraper-state.json | jq '.stacks | to_entries | map({region: .key, stack_size: (.value | length)})' 2>/dev/null || echo "Could not parse state"
          else
            echo "No state file - will start fresh with default seeds"
          fi
          echo "========================================"

      - name: Run scraper
        run: |
          DURATION=${{ github.event.inputs.duration_minutes || '45' }}
          echo "========================================"
          echo "Starting scraper for ${DURATION} minutes"
          echo "SCRAPER_THROTTLE=${SCRAPER_THROTTLE}"
          echo "Time: $(date -u)"
          echo "========================================"
          
          # Run scraper with timeout
          timeout ${DURATION}m npm run scraper || EXIT_CODE=$?
          
          if [ "${EXIT_CODE:-0}" -eq 124 ]; then
            echo "Scraper stopped by timeout (expected behavior)"
          elif [ "${EXIT_CODE:-0}" -ne 0 ]; then
            echo "Scraper exited with code $EXIT_CODE"
          fi
          
          echo "========================================"
          echo "Scraper finished at $(date -u)"
          echo "========================================"

      - name: Delete old cache
        if: always() && hashFiles('scripts/scraper-state.json') != ''
        continue-on-error: true
        run: gh cache delete scraper-state-latest || true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Save scraper state to cache
        uses: actions/cache/save@v4
        if: always() && hashFiles('scripts/scraper-state.json') != ''
        with:
          path: |
            scripts/scraper-state.json
            scripts/match-cache.json
          key: scraper-state-latest

      - name: Report status
        if: always()
        run: |
          echo "========================================"
          echo "Scraper State After Run"
          echo "========================================"
          if [ -f scripts/scraper-state.json ]; then
            echo "State file saved successfully"
            echo ""
            echo "Stack sizes by region:"
            cat scripts/scraper-state.json | jq '.stacks | to_entries | map({region: .key, stack_size: (.value | length)})' 2>/dev/null || echo "Could not parse"
            echo ""
            echo "Visited counts by region:"
            cat scripts/scraper-state.json | jq '.visited | to_entries | map({region: .key, visited: (.value | length)})' 2>/dev/null || echo "Could not parse"
          else
            echo "WARNING: No scraper state file found"
          fi
          
          echo ""
          if [ -f scripts/match-cache.json ]; then
            MATCH_COUNT=$(cat scripts/match-cache.json | jq 'length' 2>/dev/null || echo "0")
            echo "Match cache: $MATCH_COUNT known matches"
          else
            echo "WARNING: No match cache file found"
          fi
          echo "========================================"
